{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0be70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9369269609451294, Train Accuracy: 0.1071, Test Accuracy: 0.1050\n",
      "Epoch 10, Loss: 0.5323324799537659, Train Accuracy: 0.9786, Test Accuracy: 0.7970\n",
      "Epoch 20, Loss: 0.08824098110198975, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch 30, Loss: 0.02015020325779915, Train Accuracy: 1.0000, Test Accuracy: 0.8000\n",
      "Epoch 40, Loss: 0.007955827750265598, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch 50, Loss: 0.004606123082339764, Train Accuracy: 1.0000, Test Accuracy: 0.8010\n",
      "Epoch 60, Loss: 0.0033149467781186104, Train Accuracy: 1.0000, Test Accuracy: 0.7980\n",
      "Epoch 70, Loss: 0.002672974020242691, Train Accuracy: 1.0000, Test Accuracy: 0.7960\n",
      "Epoch 80, Loss: 0.002278730273246765, Train Accuracy: 1.0000, Test Accuracy: 0.7950\n",
      "Epoch 90, Loss: 0.001998076681047678, Train Accuracy: 1.0000, Test Accuracy: 0.7940\n",
      "Epoch 99, Loss: 0.0017987268511205912, Train Accuracy: 1.0000, Test Accuracy: 0.7940\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Calculate train and test accuracy\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  # Get predictions for training nodes\n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item()  # Count correct predictions\n",
    "        train_accuracy = correct_train / data.train_mask.sum().item()  # Compute accuracy\n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  # Get predictions for test nodes\n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  # Count correct predictions\n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()  # Compute accuracy\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290a459",
   "metadata": {},
   "source": [
    "I added in code to calculate the train and test accuracy of the model to help with comparisons in later problems. The train accuracy reaches 100% quickly and doesn't seem to hold much information, but I will calculate it anyway to see if there are any differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd30918",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "\n",
    "Over-smoothing occurs when the node-embeddings in a GNN get too similar to each other to the point they become indistinguishable. It often occurs when there are too many layers in the network, when nodes gain information from other nodes further away on the graph. Local nodes are no longer represented properly. There is too much message passing, and the embeddings lose meaning. The result is lower performance of the network. Adding a third layer could cause over-smoothing. Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5e0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN3Layer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(GCN3Layer, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "model3 = GCN3Layer(input_dim=dataset.num_node_features, hidden_dim1=16, hidden_dim2=8, output_dim=dataset.num_classes)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84df02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.944490909576416, Train Accuracy: 0.2071, Test Accuracy: 0.1810\n",
      "Epoch 10, Loss: 1.028019666671753, Train Accuracy: 0.6857, Test Accuracy: 0.4410\n",
      "Epoch 20, Loss: 0.29625144600868225, Train Accuracy: 1.0000, Test Accuracy: 0.7580\n",
      "Epoch 30, Loss: 0.05181198939681053, Train Accuracy: 1.0000, Test Accuracy: 0.7480\n",
      "Epoch 40, Loss: 0.010977059602737427, Train Accuracy: 1.0000, Test Accuracy: 0.7450\n",
      "Epoch 50, Loss: 0.004037094302475452, Train Accuracy: 1.0000, Test Accuracy: 0.7500\n",
      "Epoch 60, Loss: 0.0022837382275611162, Train Accuracy: 1.0000, Test Accuracy: 0.7520\n",
      "Epoch 70, Loss: 0.0016414098208770156, Train Accuracy: 1.0000, Test Accuracy: 0.7530\n",
      "Epoch 80, Loss: 0.001327091595157981, Train Accuracy: 1.0000, Test Accuracy: 0.7520\n",
      "Epoch 90, Loss: 0.0011335505405440927, Train Accuracy: 1.0000, Test Accuracy: 0.7520\n",
      "Epoch 99, Loss: 0.0010074801975861192, Train Accuracy: 1.0000, Test Accuracy: 0.7540\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model3.train()\n",
    "    optimizer3.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model3(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    model3.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item() \n",
    "        train_accuracy = correct_train / data.train_mask.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  \n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()  \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf848cec",
   "metadata": {},
   "source": [
    "Although the train accuracy was 100% and the loss was lower than the baseline model, the test accuracy was about 4% lower. This indicates that there likely was over-smoothing involved, causing the 3 layer model to be less accurate than the 2 layer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b37b76",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "\n",
    "Capacity refers to the ability of a model to learn complex patterns in data. Increasing the size of the hidden dimension increases the model's capacity. It allows for better generalization on unseen data, but only up to a point before there becomes an increased risk for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e388c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model64 = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer64 = optim.Adam(model64.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936ed493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9403634071350098, Train Accuracy: 0.1929, Test Accuracy: 0.1420\n",
      "Epoch 10, Loss: 0.06855574250221252, Train Accuracy: 1.0000, Test Accuracy: 0.8040\n",
      "Epoch 20, Loss: 0.00263330340385437, Train Accuracy: 1.0000, Test Accuracy: 0.7890\n",
      "Epoch 30, Loss: 0.0005256105796433985, Train Accuracy: 1.0000, Test Accuracy: 0.7780\n",
      "Epoch 40, Loss: 0.00023464775586035103, Train Accuracy: 1.0000, Test Accuracy: 0.7740\n",
      "Epoch 50, Loss: 0.00016375772247556597, Train Accuracy: 1.0000, Test Accuracy: 0.7740\n",
      "Epoch 60, Loss: 0.00013796996790915728, Train Accuracy: 1.0000, Test Accuracy: 0.7780\n",
      "Epoch 70, Loss: 0.00012520681775640696, Train Accuracy: 1.0000, Test Accuracy: 0.7790\n",
      "Epoch 80, Loss: 0.00011709984391927719, Train Accuracy: 1.0000, Test Accuracy: 0.7790\n",
      "Epoch 90, Loss: 0.00011094879300799221, Train Accuracy: 1.0000, Test Accuracy: 0.7790\n",
      "Epoch 99, Loss: 0.00010622364789014682, Train Accuracy: 1.0000, Test Accuracy: 0.7800\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model64.train()\n",
    "    optimizer64.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model64(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer64.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    model64.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item() \n",
    "        train_accuracy = correct_train / data.train_mask.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  \n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()   \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95b339",
   "metadata": {},
   "source": [
    "In this case, the model's capacity surely grew. It learned more complex patterns in the data, allowing the loss function to reach an order of magnitude lower than the baseline model. However, this did come with some overfitting, as the model only reached 78% accuracy on the test set as opposed to 80% on the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a96de",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f91f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNSigmoid(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNSigmoid, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "modelsig = GCNSigmoid(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizersig = optim.Adam(modelsig.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f043c3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1666195392608643, Train Accuracy: 0.1429, Test Accuracy: 0.0640\n",
      "Epoch 10, Loss: 1.5325162410736084, Train Accuracy: 0.7643, Test Accuracy: 0.5180\n",
      "Epoch 20, Loss: 1.082481861114502, Train Accuracy: 0.9714, Test Accuracy: 0.7440\n",
      "Epoch 30, Loss: 0.7212515473365784, Train Accuracy: 0.9857, Test Accuracy: 0.7900\n",
      "Epoch 40, Loss: 0.45960235595703125, Train Accuracy: 0.9857, Test Accuracy: 0.7850\n",
      "Epoch 50, Loss: 0.2960023880004883, Train Accuracy: 0.9929, Test Accuracy: 0.7800\n",
      "Epoch 60, Loss: 0.19991613924503326, Train Accuracy: 1.0000, Test Accuracy: 0.7780\n",
      "Epoch 70, Loss: 0.14317697286605835, Train Accuracy: 1.0000, Test Accuracy: 0.7740\n",
      "Epoch 80, Loss: 0.10831882804632187, Train Accuracy: 1.0000, Test Accuracy: 0.7720\n",
      "Epoch 90, Loss: 0.08572407066822052, Train Accuracy: 1.0000, Test Accuracy: 0.7700\n",
      "Epoch 99, Loss: 0.07155357301235199, Train Accuracy: 1.0000, Test Accuracy: 0.7700\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    modelsig.train()\n",
    "    optimizersig.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = modelsig(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizersig.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    modelsig.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item() \n",
    "        train_accuracy = correct_train / data.train_mask.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  \n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()   \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e12fa3",
   "metadata": {},
   "source": [
    "The performace of the model with the sigmoid function instead of ReLU did change. The loss of the sigmoid model did not reach as low as the ReLU model, and the test accuracy was also lower by about 2%. The ReLU activation function is a better choice for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3ed2f",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5052f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSmallTrainSet = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizerSmallTrainSet = optim.Adam(modelSmallTrainSet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "954d649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_nodes = data.num_nodes\n",
    "indices = np.random.permutation(num_nodes)\n",
    "split_point = int(num_nodes * 0.1)\n",
    "\n",
    "# Assign 10% to training and 90% to testing\n",
    "train_indices = indices[:split_point]\n",
    "test_indices = indices[split_point:]\n",
    "\n",
    "# Create new train and test masks (boolean masks)\n",
    "train_mask10 = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask90 = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask10[train_indices] = True\n",
    "test_mask90[test_indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a41ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9553686380386353, Train Accuracy: 0.1148, Test Accuracy: 0.1255\n",
      "Epoch 10, Loss: 0.8031471967697144, Train Accuracy: 0.9000, Test Accuracy: 0.7715\n",
      "Epoch 20, Loss: 0.23066052794456482, Train Accuracy: 0.9741, Test Accuracy: 0.8298\n",
      "Epoch 30, Loss: 0.07965433597564697, Train Accuracy: 0.9963, Test Accuracy: 0.8396\n",
      "Epoch 40, Loss: 0.037285588681697845, Train Accuracy: 0.9963, Test Accuracy: 0.8318\n",
      "Epoch 50, Loss: 0.02308228611946106, Train Accuracy: 1.0000, Test Accuracy: 0.8302\n",
      "Epoch 60, Loss: 0.016371894627809525, Train Accuracy: 1.0000, Test Accuracy: 0.8290\n",
      "Epoch 70, Loss: 0.012456296011805534, Train Accuracy: 1.0000, Test Accuracy: 0.8290\n",
      "Epoch 80, Loss: 0.009907688945531845, Train Accuracy: 1.0000, Test Accuracy: 0.8290\n",
      "Epoch 90, Loss: 0.008103243075311184, Train Accuracy: 1.0000, Test Accuracy: 0.8298\n",
      "Epoch 99, Loss: 0.006879020016640425, Train Accuracy: 1.0000, Test Accuracy: 0.8302\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    modelSmallTrainSet.train()\n",
    "    optimizerSmallTrainSet.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = modelSmallTrainSet(data)\n",
    "    loss = criterion(out[train_mask10], data.y[train_mask10])\n",
    "    loss.backward()\n",
    "    optimizerSmallTrainSet.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    modelSmallTrainSet.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[train_mask10].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[train_mask10]).sum().item() \n",
    "        train_accuracy = correct_train / train_mask10.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[test_mask90].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[test_mask90]).sum().item()  \n",
    "        test_accuracy = correct_test / test_mask90.sum().item()   \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c76f76",
   "metadata": {},
   "source": [
    "This model surprisingly performed better? The loss didn't get as low as the baseline model, reaching 0.007 at the end as opposed to 0.001, but the test accuracy was about 3.5% better. I could have done something wrong with the code. It could be that GNNs perform better when the network sees less of the graph in training (perhaps due to no over-smoothing; the representations were nicely found from the small subset of nodes), or that seeing 10% of this graph was all the GCN needed to find patterns, and more nodes in the training set only led to overfitting. I would have expected the model to perform far worse after only seeing 10% of the nodes in the graph, but I was wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21d0d",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Adam and RMSprop are similar algorithms. They both use adaptive learning rates to compute gradient descent on each parameter with step-size adjustments based on past gradients. They differ because Adam uses momentum to smooth out gradient updates and converge quicker. Based on this, I would expect the convergence speed of the model with RMSprop as its optimizer to be slower than the baseline with Adam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb23f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRMSprop = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizerRMSprop = optim.RMSprop(modelRMSprop.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f650ae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9490680694580078, Train Accuracy: 0.0786, Test Accuracy: 0.1360\n",
      "Epoch 10, Loss: 0.0547318309545517, Train Accuracy: 1.0000, Test Accuracy: 0.7850\n",
      "Epoch 20, Loss: 0.018357135355472565, Train Accuracy: 1.0000, Test Accuracy: 0.7900\n",
      "Epoch 30, Loss: 0.010021893307566643, Train Accuracy: 1.0000, Test Accuracy: 0.7870\n",
      "Epoch 40, Loss: 0.006585356779396534, Train Accuracy: 1.0000, Test Accuracy: 0.7850\n",
      "Epoch 50, Loss: 0.004774170462042093, Train Accuracy: 1.0000, Test Accuracy: 0.7860\n",
      "Epoch 60, Loss: 0.003671690821647644, Train Accuracy: 1.0000, Test Accuracy: 0.7880\n",
      "Epoch 70, Loss: 0.0029376945458352566, Train Accuracy: 1.0000, Test Accuracy: 0.7870\n",
      "Epoch 80, Loss: 0.0024171206168830395, Train Accuracy: 1.0000, Test Accuracy: 0.7860\n",
      "Epoch 90, Loss: 0.002031078329309821, Train Accuracy: 1.0000, Test Accuracy: 0.7860\n",
      "Epoch 99, Loss: 0.0017610318027436733, Train Accuracy: 1.0000, Test Accuracy: 0.7840\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    modelRMSprop.train()\n",
    "    optimizerRMSprop.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = modelRMSprop(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizerRMSprop.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    modelRMSprop.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item() \n",
    "        train_accuracy = correct_train / data.train_mask.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  \n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()   \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bf446",
   "metadata": {},
   "source": [
    "The convergence speed for the RMSprop optimizer wasn't much slower than the Adam optimizer. If anything, it reached a lower loss quicker than Adam did. By epoch 10, the loss was down to 0.05, while at epoch 10 in the baseline model, loss was 0.53. They both reached a loss under 0.01 at epoch 40 and finished with a loss around 0.018 after 100 epochs. Both models also performed roughly the same in terms of test accuracy (within 1%), meaning both are valid to use for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a30798",
   "metadata": {},
   "source": [
    "### Extra Credit 1\n",
    "\n",
    "What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "\n",
    "**Answer:** Adding edge weights to the adjacency matrix for the Cora dataset could take the form of something like how many times the connecting paper references the other. This affects message passing by adding more relevance to papers that are closely connected to each other. The message from the closely connected nodes (higher edge weights) would have a greater impact on the node's embedding than nodes more loosely connected. This could increase the strength and accuracy of the GCN, allowing it to better classify unseen nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7abf0a",
   "metadata": {},
   "source": [
    "### Extra Credit 2\n",
    "\n",
    "What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "I actually came across this issue in the last homework assignment. In creating a new class to define my neural network for one of the problems, I forgot to use the log-softmax function in the output layer, causing the loss function to go negative in training. However, that was using negative log likelihood loss instead of cross entropy loss. In consulting ChatGPT about my error, it suggested using cross entropy loss to fix my solution, as the `F.cross_entropy()` function combines both the log-softmax and negative log loss functions. This fixed my error, then I realized I could just add the log-softmax function to the output layer and continue using `F.nll_loss()` to keep things consistent, which also worked. For this problem, we are using cross entropy loss already, but this time from the torch.nn package instead of the torch.F package. I could look up documentation to see if there is any difference, but it is more fun to test and break code, so that's what I'm going to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b74077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNoLogSoftmax(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCNNoLogSoftmax, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "modelNoLogSoftmax = GCNNoLogSoftmax(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizerNoLogSoftmax = optim.Adam(modelNoLogSoftmax.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c5362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9473196268081665, Train Accuracy: 0.0857, Test Accuracy: 0.1800\n",
      "Epoch 10, Loss: 0.6583453416824341, Train Accuracy: 0.9714, Test Accuracy: 0.7810\n",
      "Epoch 20, Loss: 0.1296776980161667, Train Accuracy: 0.9929, Test Accuracy: 0.7920\n",
      "Epoch 30, Loss: 0.029255373403429985, Train Accuracy: 1.0000, Test Accuracy: 0.7880\n",
      "Epoch 40, Loss: 0.010750789195299149, Train Accuracy: 1.0000, Test Accuracy: 0.7840\n",
      "Epoch 50, Loss: 0.005935577675700188, Train Accuracy: 1.0000, Test Accuracy: 0.7840\n",
      "Epoch 60, Loss: 0.00418056920170784, Train Accuracy: 1.0000, Test Accuracy: 0.7850\n",
      "Epoch 70, Loss: 0.0033248572144657373, Train Accuracy: 1.0000, Test Accuracy: 0.7870\n",
      "Epoch 80, Loss: 0.002808221150189638, Train Accuracy: 1.0000, Test Accuracy: 0.7860\n",
      "Epoch 90, Loss: 0.0024441233836114407, Train Accuracy: 1.0000, Test Accuracy: 0.7850\n",
      "Epoch 99, Loss: 0.0021882858127355576, Train Accuracy: 1.0000, Test Accuracy: 0.7840\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    modelNoLogSoftmax.train()\n",
    "    optimizerNoLogSoftmax.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = modelNoLogSoftmax(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizerNoLogSoftmax.step()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    modelNoLogSoftmax.eval()  \n",
    "    with torch.no_grad():\n",
    "        # Training accuracy\n",
    "        _, pred_train = out[data.train_mask].max(dim=1)  \n",
    "        correct_train = (pred_train == data.y[data.train_mask]).sum().item() \n",
    "        train_accuracy = correct_train / data.train_mask.sum().item() \n",
    "\n",
    "        # Test accuracy\n",
    "        _, pred_test = out[data.test_mask].max(dim=1)  \n",
    "        correct_test = (pred_test == data.y[data.test_mask]).sum().item()  \n",
    "        test_accuracy = correct_test / data.test_mask.sum().item()   \n",
    "        \n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10c120",
   "metadata": {},
   "source": [
    "The loss function is continuting to work properly. If we were using nll loss like the last assignment, then it wouldn't work, because nll loss relies on inputs to be probabilities or log probabilities, requiring the use of the log-softmax function in the output. Cross entropy loss doesn't have the same requirement, as it can handle raw values or probabilities, so the loss in this scenario is calculated properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f18b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
