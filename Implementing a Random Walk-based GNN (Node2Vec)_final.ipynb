{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e34bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f5f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dc6d65e7674d8caeed263777976212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [01:00<00:00,  1.65it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [01:00<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4728e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_639/2447420142.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9544639587402344\n",
      "Epoch 10, Loss: 1.26720130443573\n",
      "Epoch 20, Loss: 0.9196556210517883\n",
      "Epoch 30, Loss: 0.7542466521263123\n",
      "Epoch 40, Loss: 0.674754798412323\n",
      "Epoch 50, Loss: 0.6291802525520325\n",
      "Epoch 60, Loss: 0.599407970905304\n",
      "Epoch 70, Loss: 0.5778950452804565\n",
      "Epoch 80, Loss: 0.5613651871681213\n",
      "Epoch 90, Loss: 0.5481103658676147\n",
      "Epoch 99, Loss: 0.5381905436515808\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d97392",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "\n",
    "If we increased the number of walks per node, the model could become more robust and learn the structure of the graph better. More training samples will allow the model to explore more of the neighborhood around each node and improve its embeddings. However, if the number of walks is already sufficiently high, increasing the number of walks would not improve the learned embeddings, as the entire neighborhood has already been explored, and the risk of overfitting comes into play. Let's test the model's performance on 400 walks instead of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53413b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ec9c4cafa541be85f91d004f389879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 200/200 [01:03<00:00,  3.15it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 200/200 [01:04<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec_num_walks = Node2Vec(G, dimensions=64, walk_length=30, num_walks=400, workers=2) \n",
    "model_num_walks = node2vec_num_walks.fit(window=10, min_count=1)\n",
    "embeddings_num_walks = model_num_walks.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bbc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_num_walks = Classifier(64, 7)\n",
    "optimizer_num_walks = optim.Adam(classifier_num_walks.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0c6425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1373/3569125100.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  output = classifier_num_walks(torch.tensor([embeddings_num_walks[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1075894832611084\n",
      "Epoch 10, Loss: 1.316653847694397\n",
      "Epoch 20, Loss: 0.9489686489105225\n",
      "Epoch 30, Loss: 0.7773062586784363\n",
      "Epoch 40, Loss: 0.6985472440719604\n",
      "Epoch 50, Loss: 0.6526168584823608\n",
      "Epoch 60, Loss: 0.6219195127487183\n",
      "Epoch 70, Loss: 0.5992518067359924\n",
      "Epoch 80, Loss: 0.5816271305084229\n",
      "Epoch 90, Loss: 0.5673202872276306\n",
      "Epoch 99, Loss: 0.5564438700675964\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    classifier_num_walks.train()\n",
    "    optimizer_num_walks.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier_num_walks(torch.tensor([embeddings_num_walks[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer_num_walks.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e3446",
   "metadata": {},
   "source": [
    "The loss not changing much indicates that increasing the number of walks from 200 to 400 doesn't change much in the training of the model. The neighborhood of each node is explored enough with 200 nodes, so increasing to 400 doesn't help the model improve. If anything, it overfits the model, as seen by the loss slightly increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa638e",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "If we reduce the walk length, it could influence the information in the embeddings in multiple ways, depending on how well the current length (30) captures the information in the graph. If a length of 30 is greater than the \"ideal\" length (perhaps found through tuning the walk_length hyperparameter), then reducing the walk length would help create better embeddings. Perhaps 30 is too long, and the embeddings learned using 30 as the walk length are over-smoothed, so reducing walk length would help. Too high of a walk length causes the embeddings to lose locality, as they learn about nodes that are too far away to give useful information on classification. Conversely, if 30 is less than the ideal walk length, then the embeddings already have capacity to learn more about the graph structure, and reducing it further would only lose more useful information. Let's test it with a walk length of 20 instead of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83b41fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ec3e20f22e4cfd96a7b348301f4bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:22<00:00,  4.41it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:23<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec_walk_len = Node2Vec(G, dimensions=64, walk_length=20, num_walks=200, workers=2) \n",
    "model_walk_len = node2vec_walk_len.fit(window=10, min_count=1)\n",
    "embeddings_walk_len = model_walk_len.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9522920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_walk_len = Classifier(64, 7)\n",
    "optimizer_walk_len = optim.Adam(classifier_walk_len.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d430ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9077131748199463\n",
      "Epoch 10, Loss: 1.2275166511535645\n",
      "Epoch 20, Loss: 0.8860049247741699\n",
      "Epoch 30, Loss: 0.741150438785553\n",
      "Epoch 40, Loss: 0.6704614758491516\n",
      "Epoch 50, Loss: 0.6283754110336304\n",
      "Epoch 60, Loss: 0.5997745394706726\n",
      "Epoch 70, Loss: 0.5786681175231934\n",
      "Epoch 80, Loss: 0.5622506737709045\n",
      "Epoch 90, Loss: 0.5489523410797119\n",
      "Epoch 99, Loss: 0.538888692855835\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    classifier_walk_len.train()\n",
    "    optimizer_walk_len.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier_walk_len(torch.tensor([embeddings_walk_len[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer_walk_len.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9351f",
   "metadata": {},
   "source": [
    "Since the loss remained the same with a walk length of 20, this indicates that, like the number of walks, the difference between 20 and 30 walk length isn't enough to capture any new information about the data. This could also be a result of loops and clusters in the graph, where increasing the walk length only just goes around the same loop or cluster an extra time, without adding information to the node embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342277a",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "\n",
    "The walks and embeddings would be completely different. The paths found by the algorithm would change to follow the direction of the edges instead of going in any direction. There is an argument that this is a better representation of the dataset, as papers don't cite each other, so indicating the direction of citation is useful. However, since we are trying to find similarity between papers, a node embedding losing information from the papers that cite that paper could result in a worse model. Let's test the model performance on a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ddbdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a2d2b77fe84cedb51edcf26374be8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:31<00:00,  3.13it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:32<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "G_dir = to_networkx(data, to_undirected=False)\n",
    "node2vec_dir = Node2Vec(G_dir, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model_dir = node2vec_dir.fit(window=10, min_count=1)\n",
    "embeddings_dir = model_dir.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374882b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dir = Classifier(64, 7)\n",
    "optimizer_dir = optim.Adam(classifier_dir.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96304ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_170/4156179803.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  output = classifier_dir(torch.tensor([embeddings_dir[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.891316533088684\n",
      "Epoch 10, Loss: 1.2305419445037842\n",
      "Epoch 20, Loss: 0.8907303214073181\n",
      "Epoch 30, Loss: 0.741433322429657\n",
      "Epoch 40, Loss: 0.6700621843338013\n",
      "Epoch 50, Loss: 0.6287826299667358\n",
      "Epoch 60, Loss: 0.6011267304420471\n",
      "Epoch 70, Loss: 0.5809458494186401\n",
      "Epoch 80, Loss: 0.5653074979782104\n",
      "Epoch 90, Loss: 0.5526865720748901\n",
      "Epoch 99, Loss: 0.5431882739067078\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    classifier_dir.train()\n",
    "    optimizer_dir.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier_dir(torch.tensor([embeddings_dir[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer_dir.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967440b",
   "metadata": {},
   "source": [
    "In terms of loss, this model performed basically the same as the baseline model with the undirected graph. This would indicate that directionality has no effect on the learning of embeddings in this case. Especially in the case of loops or clusters, if closely related nodes are already very connected through their citations, changing the directionality of the graph doesn't make it easier or more difficult for the random walk to represent graph structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1bd33",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "\n",
    "**Answer:** The feature vector of size 1433 in the Cora dataset is a bag-of-words vector, with each entry indicating if the word that entry represents is present in the paper. Increasing the number of words deemed significant from 1433 to 2000 could allow the model to find more nuanced relationships between the papers, and better able to determine graph structure for the embeddings, but it could also lead to overfitting. If the additional words are chosen properly to be meaningful words indicating similarity in scientific writing, the model's performance would improve. However, adding any words to the bag-of-words could include trivial words that appear in everyday language and writing. This causes the model to find relationships between papers based on authors' style of writing and the similar words they use, not indicating similarity in content of the research. Similarly, if any of the additional 567 words include stopwords - such as and, the, but, for, etc - it would only exaggerate the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b74480",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "\n",
    "**Answer:** Adding more classes would make the Random Walk GNN classification problem more difficult, and performance would likely change. The embeddings can only hold so much information, and if there are too many classes for the size of the embedding, then the difference between classes can become indistinguishable, and the classifier will struggle to make accurate predictions. Increasing the embedding dimension could help mitigate this issue to help performace, but in general, more classes would cause the classifier to become less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa5412",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8b2e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6badce7267b54892aadd79fc03b81f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:43<00:00,  2.32it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:43<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec128 = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=2) \n",
    "model128 = node2vec128.fit(window=10, min_count=1)\n",
    "embeddings128 = model128.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49edd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier128 = Classifier(128, 7)\n",
    "optimizer128 = optim.Adam(classifier128.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdeaeed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.934342622756958\n",
      "Epoch 10, Loss: 1.064994215965271\n",
      "Epoch 20, Loss: 0.7446576952934265\n",
      "Epoch 30, Loss: 0.6353906393051147\n",
      "Epoch 40, Loss: 0.5796465277671814\n",
      "Epoch 50, Loss: 0.542105495929718\n",
      "Epoch 60, Loss: 0.5147106647491455\n",
      "Epoch 70, Loss: 0.4936724007129669\n",
      "Epoch 80, Loss: 0.47655636072158813\n",
      "Epoch 90, Loss: 0.46208810806274414\n",
      "Epoch 99, Loss: 0.4507688283920288\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    classifier128.train()\n",
    "    optimizer128.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier128(torch.tensor([embeddings128[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer128.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78407b17",
   "metadata": {},
   "source": [
    "While the training time of the model slightly increased, the model's performace improved. The loss after 100 epochs of 0.45 is a noticable improvement over the loss of 0.53 that the baseline model produced. The larger embedding dimension allows the model to capture more features and decrease the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b22ec",
   "metadata": {},
   "source": [
    "### Extra Credit 1\n",
    "\n",
    "What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "Increasing the window size increases the number of neighboring nodes considered for context. Higher window size can improve the model and reduce loss, but could introduce too much noise and lead to over-smoothing. Let's test what happens when increasing window size to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116ff29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f1b3a9be7f48f2a00fd62bae65a3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:42<00:00,  2.35it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:44<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "node2vec_window = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model_window = node2vec_window.fit(window=15, min_count=1)\n",
    "embeddings_window = model_window.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c08194",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_window = Classifier(64, 7)\n",
    "optimizer_window = optim.Adam(classifier_window.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564acd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160/3794119665.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  output = classifier_window(torch.tensor([embeddings_window[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.971235752105713\n",
      "Epoch 10, Loss: 1.2834528684616089\n",
      "Epoch 20, Loss: 0.9324180483818054\n",
      "Epoch 30, Loss: 0.7695595622062683\n",
      "Epoch 40, Loss: 0.6894205808639526\n",
      "Epoch 50, Loss: 0.6443541646003723\n",
      "Epoch 60, Loss: 0.6143700480461121\n",
      "Epoch 70, Loss: 0.5927897095680237\n",
      "Epoch 80, Loss: 0.5762441158294678\n",
      "Epoch 90, Loss: 0.562949001789093\n",
      "Epoch 99, Loss: 0.5529383420944214\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    classifier_window.train()\n",
    "    optimizer_window.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier_window(torch.tensor([embeddings_window[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer_window.step()\n",
    "\n",
    "    if (epoch % 10 == 0) or (epoch == 99):\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f86f36",
   "metadata": {},
   "source": [
    "Increasing the window size also doesn't have much of an effect of loss, likely due to the structure of the graph. If what I said earlier about the loops and clusters is true, increasing the window size would also give redundant information and not help to improve the model. If anything, it reduced the embedding quality by allowing the random walk to search too far, and either adding redundant information from nearby nodes or unnecessary information from far away nodes. Either way, increasing the window size did not improve the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
